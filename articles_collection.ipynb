{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1b054e",
   "metadata": {},
   "source": [
    "# Articles Collection\n",
    "\n",
    "This notebook performs web scraping and downloads all [archived articles](https://thescipub.com/jcs/archive) in Journal of Computer Science provided by [Science Publications](https://thescipub.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c418a",
   "metadata": {},
   "source": [
    "## Web-Scraping: Retrieve Issue Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b40810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url of Science Publications web page\n",
    "main_url = 'https://thescipub.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94bc3ef",
   "metadata": {},
   "source": [
    "### Retrieve Issue Urls for all Journal Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86f10b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Issue#</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>https://thescipub.com/jcs/issue/1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>https://thescipub.com/jcs/issue/1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>https://thescipub.com/jcs/issue/1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>https://thescipub.com/jcs/issue/1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>https://thescipub.com/jcs/issue/1303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Volume  Issue#                                  urls\n",
       "0  2021      17       1  https://thescipub.com/jcs/issue/1273\n",
       "1  2021      17       2  https://thescipub.com/jcs/issue/1288\n",
       "2  2021      17       3  https://thescipub.com/jcs/issue/1292\n",
       "3  2021      17       4  https://thescipub.com/jcs/issue/1299\n",
       "4  2021      17       5  https://thescipub.com/jcs/issue/1303"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = []       # year of publication\n",
    "volumes = []     # volume number\n",
    "issues = []      # issue number in a given Volume\n",
    "urls = []        # url of Journal's issue\n",
    "\n",
    "# grab contents from the archive page for Journal of Computer Science\n",
    "page = requests.get(main_url + '/jcs/archive')\n",
    "\n",
    "# create BeautifulSoup object\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# get all divs with class 'pkp_block'\n",
    "components = soup.find_all('div', class_='pkp_block')\n",
    "\n",
    "# iterate through every div with class 'pkp_block'\n",
    "for c in components:\n",
    "    # get the text for h2 tag: yyyy - VolumeNumber\n",
    "    volume_text = c.find('h2').get_text().split(\" \")\n",
    "    \n",
    "    # get all Issue urls for a given Volume\n",
    "    links = c.find_all('a', href=True)\n",
    "    \n",
    "    # iterate through the urls\n",
    "    # and append data to the lists (years, volumes, issues, urls) for data frame creation\n",
    "    for a in links:\n",
    "        years.append(int(volume_text[0]))\n",
    "        volumes.append(int(volume_text[-1]))\n",
    "        issues.append(int(a.get_text().split(' ')[-1]))\n",
    "        urls.append(main_url + a['href'])\n",
    "\n",
    "# create a Pandas data frame\n",
    "journal_archive = pd.DataFrame(dict({'Year': years, 'Volume': volumes, 'Issue#': issues, 'urls': urls}))\n",
    "\n",
    "# display the first 5 rows of data frame\n",
    "journal_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07410f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "journal_archive.to_csv('data/journal_archive.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9564300",
   "metadata": {},
   "source": [
    "## Web-Scraping: Download Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44b6ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.93it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.07it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.61it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  3.05it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.96it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.60it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.26it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.54it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.75it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.81it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.84it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.79it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.72it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.62it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.46it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.85it/s]\n",
      "100%|██████████| 14/14 [00:03<00:00,  4.63it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  5.02it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.77it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.78it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.55it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  4.54it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  4.95it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.39it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.89it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  4.87it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.40it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.98it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.80it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.88it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.74it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.69it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.62it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.63it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.85it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.77it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.76it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.59it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.76it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.83it/s]\n",
      "100%|██████████| 14/14 [00:02<00:00,  4.92it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.49it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.09it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.53it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.97it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.33it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.92it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.20it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.77it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.98it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.85it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.62it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.76it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.80it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.47it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.77it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.15it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.92it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.74it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.20it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.15it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.64it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.49it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  5.20it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  4.70it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.92it/s]\n",
      "100%|██████████| 21/21 [00:04<00:00,  4.69it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.76it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.72it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.99it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.99it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.81it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.31it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.90it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.29it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  4.96it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.46it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.82it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.49it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.69it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.59it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.67it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.50it/s]\n",
      "100%|██████████| 30/30 [00:07<00:00,  3.96it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.83it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.68it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.65it/s]\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.54it/s]\n",
      "100%|██████████| 34/34 [00:07<00:00,  4.61it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.86it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.54it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.58it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.85it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.75it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.84it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.63it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.77it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.63it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.78it/s]\n",
      "100%|██████████| 21/21 [00:04<00:00,  4.45it/s]\n",
      "100%|██████████| 22/22 [00:04<00:00,  4.93it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.67it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.66it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.69it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.71it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.53it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.67it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.67it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.52it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.65it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.81it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.76it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.71it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.37it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.65it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.49it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "100%|██████████| 30/30 [00:06<00:00,  4.56it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.65it/s]\n",
      "100%|██████████| 26/26 [00:05<00:00,  4.42it/s]\n",
      "100%|██████████| 24/24 [00:05<00:00,  4.65it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.37it/s]\n",
      "100%|██████████| 22/22 [00:04<00:00,  4.68it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.62it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.27it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.79it/s]\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.62it/s]\n",
      "100%|██████████| 21/21 [00:04<00:00,  4.65it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.71it/s]\n",
      "100%|██████████| 19/19 [00:04<00:00,  4.53it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  4.76it/s]\n",
      "100%|██████████| 24/24 [00:05<00:00,  4.71it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.33it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.75it/s]\n",
      "100%|██████████| 31/31 [00:06<00:00,  4.62it/s]\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.71it/s]\n",
      "100%|██████████| 24/24 [00:05<00:00,  4.70it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.39it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.64it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.61it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.77it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.74it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.57it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.54it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.69it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.39it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.61it/s]\n",
      "100%|██████████| 18/18 [00:03<00:00,  4.72it/s]\n",
      "100%|██████████| 27/27 [00:06<00:00,  4.49it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 14/14 [00:03<00:00,  3.91it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  3.77it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.35it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.00it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.37it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.69it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.48it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.11it/s]\n",
      "100%|██████████| 14/14 [00:03<00:00,  4.61it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.32it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.43it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.77it/s]\n",
      "100%|██████████| 11/11 [00:02<00:00,  4.42it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.27it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n",
      "100%|██████████| 14/14 [00:03<00:00,  4.61it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.74it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:02<00:00,  4.52it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.80it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.41it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.69it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.65it/s]\n",
      "100%|██████████| 19/19 [00:04<00:00,  4.13it/s]\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.47it/s]\n",
      "100%|██████████| 14/14 [00:03<00:00,  4.58it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.34it/s]\n",
      "100%|██████████| 13/13 [00:02<00:00,  4.37it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  4.57it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.44it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  4.72it/s]\n",
      "100%|██████████| 12/12 [00:02<00:00,  4.38it/s]\n",
      "100%|██████████| 9/9 [00:02<00:00,  4.48it/s]\n",
      "100%|██████████| 6/6 [00:01<00:00,  4.32it/s]\n",
      "100%|██████████| 7/7 [00:01<00:00,  4.60it/s]\n",
      "100%|██████████| 17/17 [00:03<00:00,  4.70it/s]\n",
      "100%|██████████| 24/24 [00:05<00:00,  4.68it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.76it/s]\n",
      "100%|██████████| 15/15 [00:03<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "titles = []             # article titles\n",
    "abstracts = []          # article's abstract\n",
    "keywords = []           # article's keywords\n",
    "article_urls = []       # article's url to download pdf file\n",
    "file_names = []         # name of article's pdf file saved in local machine\n",
    "\n",
    "# iterate through every Journal Issue's url\n",
    "for i, issue_url in enumerate(urls):\n",
    "    # grab contents from a webpage\n",
    "    page = requests.get(issue_url)\n",
    "    \n",
    "    # create BeautifulSoup object\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # get all divs with class 'obj_article_summary'\n",
    "    components = soup.find_all('div', class_='obj_article_summary')\n",
    "    \n",
    "    # iterate through every div with class 'obj_article_summary'\n",
    "    # and append data to the lists (titles, article_urls, file_names) for data frame creation\n",
    "    for c in tqdm(components):\n",
    "        # get article's title\n",
    "        div_title = c.find('div', class_='title')\n",
    "        titles.append(div_title.get_text())\n",
    "        \n",
    "        # grab content of article's page\n",
    "        title_link = div_title.find('a', href=True)\n",
    "        page_1 = requests.get(main_url + title_link['href'])\n",
    "        soup_1 = BeautifulSoup(page_1.content, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            # get article's abstract\n",
    "            div_abstract = soup_1.find('div', class_='item abstract pkp_block')\n",
    "            div_abstract_p = div_abstract.find_all('p')\n",
    "            div_abstract_p = div_abstract_p if div_abstract_p is not None else div_abstract.find('p')\n",
    "            abstract_list = [div_c.get_text() for div_c in div_abstract_p]\n",
    "            abstracts.append(\" \".join(abstract_list).strip())\n",
    "        except:\n",
    "            abstracts.append('')\n",
    "        \n",
    "        try:\n",
    "            # get article's keywords\n",
    "            li_keywords = soup_1.find('ul', class_='keywords').find_all('li')\n",
    "            li_keywords = li_keywords if li_keywords is not None else soup_1.find('ul', class_='keywords').find('li')\n",
    "            keyword_list = [li_c.get_text() for li_c in li_keywords]\n",
    "            keywords.append(\", \".join(keyword_list))\n",
    "        except:\n",
    "            keywords.append('')\n",
    "        \n",
    "        # get url of pdf file\n",
    "        link = c.find('div', class_='galley_link').find('a', href=True)\n",
    "        url = main_url + link['href']\n",
    "        \n",
    "        # name of article's pdf saved in local machine\n",
    "        filename = str(years[i]) + '_' + str(volumes[i]) + '_' + str(issues[i]) + '_' + url.split('/')[-1]\n",
    "        file_names.append(filename)\n",
    "        article_urls.append(url)\n",
    "        \n",
    "        # download article and save to local machine under folder 'articles'\n",
    "        response = requests.get(url)\n",
    "        with open('articles/' + filename, 'wb') as pdf:\n",
    "            pdf.write(response.content)\n",
    "        \n",
    "        response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76186833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>File Name</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Systematic Literature Review on English and ...</td>\n",
       "      <td>Due to the enormous growth of information and ...</td>\n",
       "      <td>English Bangla Comparison, Latent Dirichlet Al...</td>\n",
       "      <td>2021_17_1_jcssp.2021.1.18.pdf</td>\n",
       "      <td>https://thescipub.com/pdf/jcssp.2021.1.18.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAD: A Detailed Arabic Dataset for Online Text...</td>\n",
       "      <td>This paper presents a novel Arabic dataset tha...</td>\n",
       "      <td>Arabic Dataset, Arabic Benchmark, Arabic Recog...</td>\n",
       "      <td>2021_17_1_jcssp.2021.19.32.pdf</td>\n",
       "      <td>https://thescipub.com/pdf/jcssp.2021.19.32.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Collision Avoidance Modelling in Airline Traff...</td>\n",
       "      <td>An Air Traffic Controller (ATC) system aims to...</td>\n",
       "      <td>Air Traffic Control, Collision Avoidance, Conf...</td>\n",
       "      <td>2021_17_1_jcssp.2021.33.43.pdf</td>\n",
       "      <td>https://thescipub.com/pdf/jcssp.2021.33.43.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fine-Tuned MobileNet Classifier for Classifica...</td>\n",
       "      <td>This paper proposed an accurate, fast and reli...</td>\n",
       "      <td>Strawberry, Cherry Fruit, Accuracy, MobileNet,...</td>\n",
       "      <td>2021_17_1_jcssp.2021.44.54.pdf</td>\n",
       "      <td>https://thescipub.com/pdf/jcssp.2021.44.54.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Content Filtering from Spam Posts on Social ...</td>\n",
       "      <td>The system for filtering spam posts on social ...</td>\n",
       "      <td>Content Filtering, Spam Detection, Multimodal ...</td>\n",
       "      <td>2021_17_1_jcssp.2021.55.66.pdf</td>\n",
       "      <td>https://thescipub.com/pdf/jcssp.2021.55.66.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  A Systematic Literature Review on English and ...   \n",
       "1  DAD: A Detailed Arabic Dataset for Online Text...   \n",
       "2  Collision Avoidance Modelling in Airline Traff...   \n",
       "3  Fine-Tuned MobileNet Classifier for Classifica...   \n",
       "4  A Content Filtering from Spam Posts on Social ...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Due to the enormous growth of information and ...   \n",
       "1  This paper presents a novel Arabic dataset tha...   \n",
       "2  An Air Traffic Controller (ATC) system aims to...   \n",
       "3  This paper proposed an accurate, fast and reli...   \n",
       "4  The system for filtering spam posts on social ...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  English Bangla Comparison, Latent Dirichlet Al...   \n",
       "1  Arabic Dataset, Arabic Benchmark, Arabic Recog...   \n",
       "2  Air Traffic Control, Collision Avoidance, Conf...   \n",
       "3  Strawberry, Cherry Fruit, Accuracy, MobileNet,...   \n",
       "4  Content Filtering, Spam Detection, Multimodal ...   \n",
       "\n",
       "                        File Name  \\\n",
       "0   2021_17_1_jcssp.2021.1.18.pdf   \n",
       "1  2021_17_1_jcssp.2021.19.32.pdf   \n",
       "2  2021_17_1_jcssp.2021.33.43.pdf   \n",
       "3  2021_17_1_jcssp.2021.44.54.pdf   \n",
       "4  2021_17_1_jcssp.2021.55.66.pdf   \n",
       "\n",
       "                                              URL  \n",
       "0   https://thescipub.com/pdf/jcssp.2021.1.18.pdf  \n",
       "1  https://thescipub.com/pdf/jcssp.2021.19.32.pdf  \n",
       "2  https://thescipub.com/pdf/jcssp.2021.33.43.pdf  \n",
       "3  https://thescipub.com/pdf/jcssp.2021.44.54.pdf  \n",
       "4  https://thescipub.com/pdf/jcssp.2021.55.66.pdf  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a data frame for articles\n",
    "articles = pd.DataFrame(dict({'Title': titles, 'Abstract': abstracts, 'Keywords': keywords,\n",
    "                              'File Name': file_names, 'URL': article_urls}))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a383a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save articles as csv file\n",
    "articles.to_csv('data/articles.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
